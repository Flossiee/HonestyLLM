# Note: DO NOT use quantized model or quantization_bit when merging lora adapters

# model
model_name_or_path:  meta-llama/Meta-Llama-3-8B-Instruct #mistralai/Mistral-7B-Instruct-v0.2 
adapter_name_or_path: path_to_stage1_lora_adapter
template: mistral #llama3/mistral
finetuning_type: lora

# export
export_dir: models/path_to_export_merged_model
export_size: 4
export_device: cpu
export_legacy_format: false






